import asyncio
import os
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMExtractionStrategy, LLMConfig
from dotenv import load_dotenv
from datetime import datetime

load_dotenv()

# Definindo o schema para os dados.
class DatasetMetadata(BaseModel):
    is_opendata_page: bool = Field(..., description="Indica se a p√°gina √© de dados abertos.")
    last_updated_date: str = Field(..., description="Data da √∫ltima atualiza√ß√£o do dataset, ou 'N√£o encontrado'")
    version_info: str = Field(..., description="Informa√ß√µes de vers√£o do dataset, ou 'N√£o encontrado'")
    license_info: str = Field(..., description="Informa√ß√µes sobre a licen√ßa (ex: Creative Commons), ou 'N√£o encontrado'")
    data_formats: list[str] = Field(..., description="Lista de formatos de arquivo dispon√≠veis (ex: CSV, XLSX), ou ['N√£o encontrado']")
    main_page_url: str = Field(..., description="URL da p√°gina principal do dataset, ou 'N√£o encontrado'")
    url_download_data: str = Field(..., description="URL para download dos dados, ou 'N√£o encontrado'")
    possible_api: str = Field(..., description="Poss√≠veis URLs de API para acesso aos dados, ou 'N√£o encontrado'")
    presencaURLsArquivosMetadadosAnexos: str = Field(..., description="URLs de arquivos, metadados ou anexos presentes na p√°gina, ou 'N√£o encontrado'")
    URLsArquivosMetadadosAnexos: list[str] = Field(..., description="Lista de URLs de arquivos, metadados ou anexos presentes na p√°gina, ou ['N√£o encontrado']")
    title: str = Field(..., description="T√≠tulo do dataset, ou 'N√£o encontrado'")
    descricao: str = Field(..., description="Descri√ß√£o do dataset, ou 'N√£o encontrado'")
    emails: list[str] = Field(..., description="Lista de e-mails de contato, ou ['N√£o encontrado']")
    responsable_organizacao: str = Field(..., description="Nome da organiza√ß√£o respons√°vel pelo dataset, ou 'N√£o encontrado'")


async def main():
    groq_api_key = os.getenv("GROQ_API_KEY")
    if not groq_api_key:
        print(" GROQ_API_KEY n√£o encontrado no .env")
        return
    
    url = input("Insira a URL da p√°gina de dados: ").strip()
    if not url:
        print(" URL inv√°lida.")
        return
    
    # Configura√ß√£o da extra√ß√£o com LLM
    crawler_config = CrawlerRunConfig(
        cache_mode="bypass",
        extraction_strategy=LLMExtractionStrategy(
            llm_config=LLMConfig(
                provider="groq/llama3-8b-8192",
                api_token=groq_api_key,
            ),
            schema=DatasetMetadata.model_json_schema(),
            extraction_type="schema",
            instruction=(
                "Retorne exclusivamente um JSON v√°lido, sem explica√ß√µes, sem coment√°rios. Extraia da p√°gina as seguintes informa√ß√µes:\n"
                "0. Indica se a p√°gina √© de dados abertos. Se n√£o for, pode parar por aqui o crawl.\n"
                "1. Data da √∫ltima atualiza√ß√£o do dataset.\n"
                "2. Informa√ß√µes de vers√£o.\n"
                "3. Tipo de licen√ßa.\n"
                "4. Lista de formatos de arquivo dispon√≠veis.\n"
                "5. Url da p√°gina principal.\n"
                "6. Url para download dos dados.\n"
                "7. Poss√≠veis URLs de API para acesso aos dados.\n"
                "8. URLs de arquivos, metadados ou anexos presentes na p√°gina.\n"
                "9. T√≠tulo do dataset.\n"
                "10. Descri√ß√£o do dataset.\n"
                "11. Lista de e-mails de contato.\n"
                "12. Nome da organiza√ß√£o respons√°vel pelo dataset.\n"
                "Se algum item n√£o for encontrado, retorne 'N√£o encontrado'."
            ),
            extra_args={"temperature": 0, "max_tokens": 1000},
        ),
    )

    print(f"\nüåê Iniciando crawl em {url}\n")
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url, config=crawler_config)

        if result.success:
            print("Extra√ß√£o conclu√≠da com sucesso:")
            print(result.extracted_content) 
        else:
            print("Falha no crawl:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
